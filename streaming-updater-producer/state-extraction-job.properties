# job name for boostrap job
job_name: "WDQS Updater State Extraction Job"

# checkpoints path (FIXME: we should not need checkpoints for this)
checkpoint_dir: hdfs://analytics-hadoop/wmf/discovery/streaming_updater/checkpoints

##
# savepoint/checkpoint to read
input_savepoint: hdfs://analytics-hadoop/wmf/discovery/streaming_updater/savepoint/20201005

##
# verify if the savepoint have any buffered events and fail
verify: true

##
# prefixes to append to all the input topic names, to support the WMF multi-DC layout of the kafka topics
topic_prefixes: eqiad.,codfw.

##
# input topics
rev_create_topic: mediawiki.revision-create
page_delete_topic: mediawiki.page-delete
suppressed_delete_topic: mediawiki.page-suppress
page_undelete_topic: mediawiki.page-undelete

##
# path to store the csv files with kafka offsets (one file per topic)
# when unset consumer states won't be extracted
# kafka_offsets_output: hdfs://analytics-hadoop/wmf/data/discovery/wdqs/kafka_offsets/20201005

##
# path to write the entity -> revision csv file
rev_map_output: hdfs://analytics-hadoop/wmf/data/discovery/wdqs/entity_revision_map/20201005/rev_map.csv