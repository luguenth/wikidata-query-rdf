##
# hostname for which the updater should capture the events for (filtering events based on meta.domain)
hostname: www.wikidata.org

##
# Job name for the updater pipeline
job_name: "WDQS Streaming Updater POC"

##
# custom routes to use to connect to various http services, default: none
# format is hostname1:https://target1:port,hostname2:https://target2:port
# http_routes: hostname1:https://target1:port,hostname2:https://target2:port

##
# http client timeout
# timeout for http connection: connectTime, readTime, soTime
# default: 5000 (millisec)
# http_timeout: 5000

##
# http user-agent
# defaults to "Wikidata Query Service Updater Bot"
# user_agent: Wikidata Query Service Updater Bot

##
# schema_repositories
# defaults to https://schema.wikimedia.org/repositories/primary/jsonschema,https://schema.wikimedia.org/repositories/secondary/jsonschema
# schema_repositories: https://schema.wikimedia.org/repositories/primary/jsonschema,https://schema.wikimedia.org/repositories/secondary/jsonschema

##
# namespaces of the entities the pipeline must capture events for
# 0: main, 120: properties, 146: lexemes
entity_namespaces: 0,120,146

##
# path to the generated checkpoints
checkpoint_dir: hdfs://analytics-hadoop/wmf/discovery/streaming_updater/checkpoints

##
# kafka brokers used to consume and produce messages
brokers: kafka-jumbo1001.eqiad.wmnet:9092

##
# prefixes to append to all the input topic names, to support the WMF multi-DC layout of the kafka topics
topic_prefixes: eqiad.,codfw.

##
# input topics
rev_create_topic: mediawiki.revision-create
page_delete_topic: mediawiki.page-delete
suppressed_delete_topic: mediawiki.page-suppress
page_undelete_topic: mediawiki.page-undelete

##
# kafka consumer group, used to fetch the initial set of offset then only used for monitoring
consumer_group: wdqs_streaming_updater_changeme

##
# path to store "spurious" events, events that are inconsistent with the state of the application
# optional: comment to use kafka & event streams for this data
spurious_events_dir: hdfs://analytics-hadoop/wmf/discovery/streaming_updater/spurious_events

##
# path to events considered "late" with the event reordering operation
# optional: comment to use kafka & event streams for this data
late_events_dir: hdfs://analytics-hadoop/wmf/discovery/streaming_updater/late_events

##
# path to events we were unable to fetch the data for
# optional: comment to use kafka & event streams for this data
failed_ops_dir: hdfs://analytics-hadoop/wmf/discovery/streaming_updater/failed_events

##
# endpoint to fetch the configuration for WMF event streams
# only used for side outputs running through kafka
event_stream_config_endpoint: https://meta.wikimedia.org/w/api.php

##
# output topic prefix (must be the current datacenter)
# only used by side outputs for now
output_topic_prefix: datacenter-changeme.

##
# domain to set in the meta subfield for side output events
# useful to distinguish these events when running multiple pipelines for the same
# wikibase instance
# defaults to the hostname property
# side_outputs_domain: custom.wikimedia.org

##
# kafka broker to use for side outputs
# optional, defaults to brokers
# side_outputs_kafka_brokers: kafka-jumbo1001.eqiad.wmnet:9092

##
# window length (ms) for the reordering operation
# reordering_window_length: 60000

##
# enforce general parallelism (excluding output operations that have to go with the pararellism 1
parallelism: 2

##
# timeout for the generate diff async IO (defaults to 5minute) -1 to disable
generate_diff_timeout: -1

##
# thead pool size for the generate diff async IO (defaults to 30)
wikibase_repo_thread_pool_size: 30

##
# path to fetch the entity data (defaults to /wiki/Special:EntityData/)
# wikibase_entitydata_path: /wiki/Special:EntityData/

##
# output topic
output_topic: wdqs_streaming_updater_changeme

##
# output topic partition
output_topic_partition: 0

##
# idleness (ms) of input sources, default to 1minute
#
# - a value that is too low (< 1s) may incorrectly detect the stream as idle
# and might jump to higher watermarks issued from low rate streams and will end
# up marking as late events most events of this stream
#
# - a value that is too high (>?) may cause backfills to fail because too many
# events may be buffered in the window operation while idleness is reached.
# Later these events will likely be triggered at the same time causing too much
# backpressure and will cause the checkpoint to fail.
# (bug? some thread being blocked on
#  org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegmentBlocking)
input_idleness: 2000

##
# max lateness (ms): allowed out-of-orderness
# max_lateness: 60000

##
# checkpoint_interval (ms)
checkpoint_interval: 30000

##
# checkpoint timeouts
checkpoint_timeout: 900000

##
# enable unaligned checkpoints, defaults to false
# unaligned_checkpoints: false

##
# min pause (ms) between checkpoints, defaults to 2seconds
# min_pause_between_checkpoints: 2000

##
# watermark intervals (ms), defaults to 200millis
# auto_wm_interval: 200

##
# enable/disable exactly_once, defaults to true
# exactly_once: true

##
# enable flink latency tracking (defaults disabled), in millisec
# latency_tracking_interval: 1000

##
# tune network buffer timeout (ms)
# network_buffer_timeout: 100

##
# configure the failure rate restart strategy
# https://ci.apache.org/projects/flink/flink-docs-stable/dev/task_failure_recovery.html#failure-rate-restart-strategy
# 2 failures max per 30min (allows failing once for the time of 2 * checkpoint timeouts)
restart_failures_rate_interval: 1800000
restart_failures_rate_max_per_interval: 2
restart_failures_rate_delay: 10000
