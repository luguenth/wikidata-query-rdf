# job name for boostrap job
job_name: "WDQS Updater Bootstrap Job"

# checkpoints path (FIXME: we should not need checkpoints for this)
checkpoint_dir: hdfs://analytics-hadoop/wmf/discovery/streaming_updater/checkpoints

# path to the entity -> revision map (CSV file)
# data generated by the rdf-spark-utils submodule and automated with
# airflow/dags/import_wikidata_ttl.py from
# https://gerrit.wikimedia.org/g/wikimedia/discovery/analytics
revisions_file: hdfs://analytics-hadoop/wmf/data/discovery/wdqs/entity_revision_map/20201005/rev_map.csv

# path to the generated save-point
savepoint_dir: hdfs://analytics-hadoop/wmf/discovery/streaming_updater/savepoint/20201005

# parallelism of the decide mutation operator
parallelism: 10

