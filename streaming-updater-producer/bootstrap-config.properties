# job name for boostrap job
job_name: "WDQS Updater Bootstrap Job"

# checkpoints path (FIXME: we should not need checkpoints for this)
checkpoint_dir: hdfs://analytics-hadoop/wmf/discovery/streaming_updater/checkpoints

##
# prefixes to append to all the input topic names, to support the WMF multi-DC layout of the kafka topics
topic_prefixes: eqiad.,codfw.

##
# input topics
rev_create_topic: mediawiki.revision-create
page_delete_topic: mediawiki.page-delete
suppressed_delete_topic: mediawiki.page-suppress
page_undelete_topic: mediawiki.page-undelete

##
# path from where csv files holding kafka offsets are stored
# when unset consumers will start from where they are
# kafka_offsets_folder: hdfs://analytics-hadoop/wmf/data/discovery/wdqs/kafka_offsets/20201005

# path to the entity -> revision map (CSV file)
# data generated by the rdf-spark-utils submodule and automated with
# airflow/dags/import_wikidata_ttl.py from
# https://gerrit.wikimedia.org/g/wikimedia/discovery/analytics
revisions_file: hdfs://analytics-hadoop/wmf/data/discovery/wdqs/entity_revision_map/20201005/rev_map.csv

# path to the generated save-point
savepoint_dir: hdfs://analytics-hadoop/wmf/discovery/streaming_updater/savepoint/20201005

# parallelism of the decide mutation operator
parallelism: 10

